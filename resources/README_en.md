# WisdoMentor
<p align="center"> <img src="resources/title_logo.svg" style="width: 40%;" id="title-icon">  </p>
<!-- <p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center"> -->
<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">
<a href="" target="_blank" style="margin-left: 6px">ğŸ¤—</a> <a href="" target="_blank" style="margin-left: 6px">HuggingFace</a>  â€¢ | 
<a href="" target="_blank" style="margin-left: 10px">ğŸ¤–</a> <a href="" target="_blank" style="margin-left: 6px">ModelScope</a>  â€¢ |
<a href="" target="_blank" style="margin-left: 10px">ğŸ“ƒ</a> <a href="https://arxiv.org" target="_blank" style="margin-left: 6px">[Wisdom-8B @ arxiv]</a>
  
</p>

<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">
ğŸ­ <a href="http://wisdomentor.jludreamworks.com" target="_blank"  style="margin-left: 6px">Try WisdoMentor</a> â€¢ |
<a href="" target="_blank" style="margin-left: 10px">ğŸ’¬</a> <a href="./resources/wechat.jpg" target="_blank"  style="margin-left: 6px">WeChat</a> 
</p>

<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">
<a href="./resources/README_en.md" target="_blank"  style="margin-left: 6px">English Readme</a>  â€¢ |
<a href="./README.md" target="_blank"  style="margin-left: 6px">ä¸­æ–‡ Readme</a> 
</p>



# Table of Contents
- [ğŸ“° News Update](#News_Update)
- [ğŸ“š Model Introduction](#Model_Introduction)
- [ğŸ“Š Benchmark ResultsğŸ†ğŸ†ğŸ†](#Benchmark_Results)
- [ğŸŒ Inference and Deployment](#Inference_and_Deployment)
- [ğŸ“• Disclaimer and License](#Disclaimer_and_License)



# News_Update

[4/19/2024] ğŸ”¥ğŸ”¥ğŸ”¥**We have released the first version of WidsoMenter-8B model: Wisdom Block Extension. It includes more SFT data generated from blogs and injects some SFT data in Stage CPT.**

[4/11/2024] ğŸ”¥ğŸ”¥ğŸ”¥**We have released the first version of WidsoMenter-7B model.**

# æ¨¡å‹ä»‹ç»

- WidsoMentorå‰æ¢¦æ™ºåˆ›æ¨å‡ºçš„**é›¶åŸºç¡€AIè¾…åŠ©æ•™è‚²å¤§æ¨¡å‹**ï¼Œé‡‡ç”¨**250ä¸‡**ç¯‡Arxivé«˜è´¨é‡äººå·¥æ™ºèƒ½è®ºæ–‡è®­ç»ƒã€‚
- é‡‡ç”¨**Bonito Instruct**ã€**Self Instruct**ã€**Involve Instruct**ç­‰å¤šç§æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡é—¨æ§æŠ€æœ¯å®ç°å¤šæ–¹æ³•çš„æœ‰æœºèåˆã€‚
- åµŒå…¥**RAG**æŠ€æœ¯ï¼Œä¿è¯WisdoMentorå›ç­”çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚
- æ¥å…¥**Agent**æ–¹æ³•ï¼Œåœ¨å›ç­”ä¸­åµŒå…¥å¯ä»¥è¢«å‚é˜…çš„é«˜è´¨é‡å›ç­”ç½‘é¡µï¼Œæä¾›å›ç­”ä¹‹å¤–æ›´å¤šçš„çŸ¥è¯†ç»†èŠ‚ã€‚

# Model Introduction
- WidsoMentor, developed by JiMengZhiChuang, is a zero-based AI-assisted education mega-model trained on 2.5 million high-quality research papers from Arxiv in the field of artificial intelligence.
- It incorporates various instruction generation methods such as Bonito Instruct, Self Instruct, and Involve Instruct, achieving an organic fusion of multiple approaches through gating techniques.
- It embeds RAG (Retrieval-Augmented Generation) technology to ensure the accuracy and timeliness of WidsoMentor's responses.
- It adopts the Agent approach to integrate high-quality answer webpages that can be referenced within the answers, providing additional knowledge details beyond the responses.

# Benchmark ç»“æœ

æˆ‘ä»¬åœ¨[é€šç”¨](#é€šç”¨é¢†åŸŸ)ã€[æ•°å­¦](#æ•°å­¦)ç­‰å¤šä¸ªé¢†åŸŸçš„æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•

## é€šç”¨é¢†åŸŸ

æˆ‘ä»¬å¯¹WisdoMentoråˆ†åˆ«åœ¨é€šç”¨é¢†åŸŸæ•°æ®é›†C-Evalã€MMLUã€CMMLUç­‰ä¸‰ä¸ªæƒå¨æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œåˆ†åˆ«æ¶µç›–äº†å…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹ã€è‹±æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹å’Œä¸­æ–‡è¯­å¢ƒä¸‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

### 7B æ¨¡å‹ç»“æœ

|                          | **C-Eval** | **MMLU** | **CMMLU** |
|:------------------------:|:----------:|:--------:|:---------:|
|                          |  5-shot    |  5-shot  |  5-shot   |
| **GPT-4**                | 68.40      | 83.93    | 70.33     |
| **GPT-3.5 Turbo**        | 51.10      | 68.54    | 54.06     |
| **LLaMA-7B**             | 27.10      | 35.10    | 26.75     |
| **LLaMA2-7B**            | 28.90      | 45.73    | 31.38     |
| **MPT-7B**               | 27.15      | 27.93    | 26.00     |
| **Falcon-7B**            | 24.23      | 26.03    | 25.66     |
| **ChatGLM2-6B**          | 50.20      | 45.90    | 49.00     |
| **WisdoMentor-8B**       |            |          |           | 

## æ•°å­¦
## ä»£ç 

----

# æ¨ç†å’Œéƒ¨ç½²

æ¥ä¸‹æ¥æˆ‘ä»¬å±•ç¤ºä½¿ç”¨ [FastChat](https://github.com/lm-sys/FastChat)ï¼Œ[Transformers](#import-from-transformers)ï¼Œ[ModelScope](#import-from-modelscope) å’Œ [Web demo](#dialogue) è¿›è¡Œæ¨ç†ã€‚
å¯¹è¯æ¨¡å‹é‡‡ç”¨äº† [chatml æ ¼å¼](./chat/chat_format.md) æ¥æ”¯æŒé€šç”¨å¯¹è¯å’Œæ™ºèƒ½ä½“åº”ç”¨ã€‚
ä¸ºäº†ä¿éšœæ›´å¥½çš„ä½¿ç”¨æ•ˆæœï¼Œåœ¨ç”¨ [Transformers](#import-from-transformers) æˆ– [ModelScope](#import-from-modelscope) è¿›è¡Œæ¨ç†å‰ï¼Œè¯·æŒ‰å¦‚ä¸‹æŒ‡ä»¤å®‰è£…ä¾èµ–ã€‚

### å®‰è£…ä¾èµ–

```shell
git clone https://www.modelscope.cn/linjh1118/WisdoMentor-8b
conda create --name WisdoMentor python=3.11.8
conda activate WisdoMentor
pip install -r requirements.txt
```

### é€šè¿‡ FastChat éƒ¨ç½²æ¨ç†
```python
git clone https://www.modelscope.cn/linjh1118/WisdoMentor-8b path_to_local_WisdoMentor-8b
cd path_to_local_WisdoMentor-8b
python -m fastchat.serve.cli --model-path path_to_local_WisdoMentor-8b
é—®: ä»‹ç»ä¸‹bertå’Œgptæœ‰ä»€ä¹ˆåŒºåˆ«
ç­”: Bert (Bidirectional Encoder Representations from Transformers) å’Œ GPT (Generative Pre-trained Transformers)éƒ½æ˜¯é¢„è®­ç»ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œä½†å®ƒä»¬çš„é¢„è®­ç»ƒä»»åŠ¡å’Œåº”ç”¨åœºæ™¯æœ‰æ‰€ä¸åŒã€‚ Berté€šè¿‡åŒå‘çš„ç¼–ç å™¨ç»“æ„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿæ•æ‰åˆ°å¥å­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…·æœ‰éå¸¸å¥½çš„è¯­è¨€ç†è§£å’Œè¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚Berté¢„è®­ç»ƒçš„ä»»åŠ¡æ˜¯é€šè¿‡å°†å¥å­çš„ä¸¤éƒ¨åˆ†åˆ†åˆ«ç”¨åŒè¯­æ ‡è®°ï¼Œç„¶åé¢„æµ‹è¿™ä¸¤ä¸ªéƒ¨åˆ†ä¹‹é—´çš„å…³ç³»æ¥å®Œæˆçš„ã€‚Bertåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æ–‡æœ¬åŒ¹é…ã€é—®ç­”ç³»ç»Ÿå’Œæ–‡æœ¬æ‘˜è¦ç­‰ã€‚ GPTæ˜¯åŸºäºå•å‘çš„ç¼–ç å™¨ç»“æ„ï¼Œä¸BERTä¸åŒã€‚GPTé¢„è®­ç»ƒçš„ä»»åŠ¡æ˜¯é€šè¿‡å°†æ–‡æœ¬ä¸­çš„å•è¯å’Œå¥å­åˆ†åˆ«æ ‡è¯†ï¼Œç„¶åé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ¥å®Œæˆçš„ã€‚GPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚ è™½ç„¶Bertå’ŒGPTé¢„è®­ç»ƒçš„ä»»åŠ¡ä¸åŒï¼Œä½†å®ƒä»¬éƒ½æ˜¯é¢„è®­ç»ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œåœ¨å¤„ç†ç‰¹å®šä»»åŠ¡æ—¶å¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚é€‰æ‹©ä½¿ç”¨BERTè¿˜æ˜¯GPTå–å†³äºå…·ä½“ä»»åŠ¡çš„éœ€æ±‚å’Œç›®æ ‡ã€‚
```

### é€šè¿‡ ModelScope åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» ModelScope åŠ è½½ WisdoMentor-8b æ¨¡å‹ ï¼ˆå¯æ ¹æ®æœ¬åœ°ç®—åŠ›æ¡ä»¶ï¼Œä¿®æ”¹æ¨¡å‹åç§°ï¼Œæ›¿æ¢æˆä¸åŒå°ºå¯¸çš„WisdoMentorï¼‰

```python
import torch
from modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM
model_dir = snapshot_download('linjh1118/WisdoMentor-8b')
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)
model = model.eval()
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸‹Bertå’ŒGPTçš„åŒºåˆ«", history=[])
print(response)
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸‹Self-Attentionæœºåˆ¶", history=history)
print(response)
```

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» Transformers åŠ è½½ WisdoMentor-8b æ¨¡å‹ ï¼ˆå¯æ ¹æ®æœ¬åœ°ç®—åŠ›æ¡ä»¶ï¼Œä¿®æ”¹æ¨¡å‹åç§°ï¼Œæ›¿æ¢æˆä¸åŒå°ºå¯¸çš„WisdoMentorï¼‰

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("linjh1118/WisdoMentor-8b", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("linjh1118/WisdoMentor-8b", device_map="auto",trust_remote_code=True, torch_dtype=torch.float16)
# 4-bit é‡åŒ–
# pip install -U bitsandbytes
# 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_8bit=True)
# 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_4bit=True)
model = model.eval()
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸‹Bertå’ŒGPTçš„åŒºåˆ«", history=[])
print(response)
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸‹Self-Attentionæœºåˆ¶", history=history)
print(response)
```


# å£°æ˜ã€åè®®

## å£°æ˜

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œæˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿå¹¶æœªåŸºäº WidsoMentor æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ï¼Œæ— è®ºæ˜¯åœ¨ iOSã€Androidã€ç½‘é¡µæˆ–ä»»ä½•å…¶ä»–å¹³å°ã€‚æˆ‘ä»¬å¼ºçƒˆå‘¼åæ‰€æœ‰ä½¿ç”¨è€…ï¼Œä¸è¦åˆ©ç”¨ WidsoMentor æ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† WidsoMentor æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

## åè®®
ç¤¾åŒºä½¿ç”¨ WidsoMentor æ¨¡å‹éœ€è¦éµå¾ª [Apache 2.0](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE)ã€‚WidsoMentor æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°† WidsoMentor æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„ä¸»ä½“ç¬¦åˆä»¥ä¸‹æƒ…å†µï¼š
  1. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹çš„æœåŠ¡æˆ–äº§å“çš„æ—¥å‡ç”¨æˆ·æ´»è·ƒé‡ï¼ˆDAUï¼‰ä½äº100ä¸‡ã€‚
  2. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸æ˜¯è½¯ä»¶æœåŠ¡æä¾›å•†ã€äº‘æœåŠ¡æä¾›å•†ã€‚
  3. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸å­˜åœ¨å°†æˆäºˆæ‚¨çš„å•†ç”¨è®¸å¯ï¼Œæœªç»ç™¾å·è®¸å¯äºŒæ¬¡æˆæƒç»™å…¶ä»–ç¬¬ä¸‰æ–¹çš„å¯èƒ½ã€‚


